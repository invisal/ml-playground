{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Principle Component Analysis With Singular Value Decomposition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By Rina Buoy, etc.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Any dataset with a number of dimensions (features) greater than 3 is visually challenging due to our inability to visualise more than 3D and it is not possible for us to control the number of dimensions of raw data. However, we can control our projection which allows us to represent high-dimension data in low-dimension equivalence. In addition, not all dimensions are equal; some are more correlated with one another than others. We can collapse (or project) a set of correlated dimensions into one equivalent dimension without losing significant information. How can we achieve this? The answer is Principle Component Analysis ( PCA in short). PCA is one of the familiar terms in the data science field. To view PCA simply as dimension reduction is a misconception because it does far more than that. We should not use PCA to collapse dimensions just because of our inability to view high dimension but we should use PCA because those dimensions are collapsable without losing significant (or desired) information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Formal Definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accorindly to Wikipedia:\n",
    "\n",
    "\"Principal component analysis (PCA) is a statistical procedure that uses an orthogonal transformation to convert a set of observations of possibly correlated variables (entities each of which takes on various numerical values) into a set of values of linearly uncorrelated variables called principal components.\"\n",
    "\n",
    "The initial reaction to the above definition whould be \"what the heck?\". \n",
    "\n",
    "From my experience, any formal definition of mathematical or statistical theorems is hard to comprehend and intimidating at first. It is not due to that those definitions are vague or ambiguous but it is mostly due to our initial lack of understanding of the subject matter. Having mastered and understood the subject, we will appreciate the succintness of formal definition. \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computations of PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assume that we have a raw data with two dimensions (x1 and x2), the cross-plot of wich is given below. It can be observed that the variables are highly correlated. Recalling the above definition of PCA, we need to convert these correlated points into a set of uncorrelated variables. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='image_1.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To do that, we first draw a vector from the center of mass poiting in the direction of the largest variance. This vector is the 1st principle component - P1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='image_2.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we need to find the second principle component - P2. Since the principle components are uncorrelated, P2 must be orthogonal to P1. P1 and P2 are orthogonal basis for new coordinate and are linearly independent. For curious readers, please to 'Introduction to Linear Algebra' by Gilbert Strang for the topics of orthogonality, linear independence, and basis vector. Therefore, P2 is a vector from the center of mass poiting in the direction orthogonal to P1. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='image_3.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we just did manual PCA calcualtions involving two principle components. From the above plot, the variance in the direction pointed by P2 is far smaller than that pointed by P1; therefore, we can drop P2 and project all points orthogonally onto P1 while retaining as much information as possible. We now see the power of orthogonal transformation of PCA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Formally, PCA attempts to perform orthogonal project of high-dimension data onto low-dimension linear space, which:\n",
    "\n",
    "1-maximizes variance of projected data (purple line)\n",
    "\n",
    "2-minimizes mean squared distance between data point and projections (sum of blue lines)\n",
    "\n",
    "Point 2 is basically the princple of least-squared regression. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='image_4.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It turns out that all principle components can be obtained from the results of Singular Value Decomposition (SVD). We discuss SVD and how principle components are derived from SVD results in the following section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Singular Value Decomposition (Visal ?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCA Applications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image Compression (Kevin ? ref: L12.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We take image with noisy background, apply svd and then truncate. We need to illustrate how momory are saved. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image Denoising (Kevin ? ref: PCA_24_10_2009.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We take image with noisy background, apply svd and then truncate small eigen values components to denoise. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lower-Dimension Visualization (Chamnab ? ref: L13.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From high-dimension data which cannot be visualised easily, we apply pca to reduce dimensions and vizualize. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA Classification/Regression (Chamnab?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part, we take a dataset for regression/classification and apply pca to reduce dimensions and using dimension-reduced dataset for regression/classification. We then compare the accuracy between original vs dimension-reduced dataset to demonstrate the robustness of PCA. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
